{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Denosing using RNN # \n",
    "\n",
    "This problem uses RNN as a tool for Speech Denoising purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.contrib import rnn\n",
    "import glob\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Training Set ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ LOADING OF TRAINING DATA #################\n",
    "\n",
    "train = []\n",
    "signal = []\n",
    "noise = []\n",
    "X_list = []\n",
    "\n",
    "import os, re\n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/tr/trx*\")):\n",
    "        st, sr=librosa.load(f, sr=None)\n",
    "        X=librosa.stft(st, n_fft=1024, hop_length=512)\n",
    "        X_abs = np.abs(X)\n",
    "        train.append(np.transpose(X_abs))\n",
    "        X_list.append(np.transpose(X))\n",
    "        \n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/tr/trn*\")):\n",
    "        sn, sr=librosa.load(f, sr=None)\n",
    "        X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        X_abs = np.abs(X)\n",
    "        noise.append(np.transpose(X_abs))\n",
    "        \n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/tr/trs*\")):\n",
    "        sc, sr=librosa.load(f, sr=None)\n",
    "        X=librosa.stft(sc, n_fft=1024, hop_length=512)\n",
    "        X_abs = np.abs(X)\n",
    "        signal.append(np.transpose(X_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### SAVING THE FILES AS PICKLE ############\n",
    "# with open('train.pkl', 'wb') as f:\n",
    "#      pickle.dump(train, f)\n",
    "    \n",
    "# with open('noise.pkl', 'wb') as f:\n",
    "#      pickle.dump(noise, f)\n",
    "    \n",
    "# with open('signal.pkl', 'wb') as f:\n",
    "#      pickle.dump(signal, f)\n",
    "        \n",
    "# with open('X_list.pkl', 'wb') as f:\n",
    "#      pickle.dump(X_list, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ LOADING OF PICKLE FILES ############\n",
    "\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    train1 = pickle.load(f)\n",
    "    \n",
    "with open('noise.pkl', 'rb') as f:\n",
    "    noise1 = pickle.load(f)\n",
    "    \n",
    "with open('signal.pkl', 'rb') as f:\n",
    "    signal1 = pickle.load(f)\n",
    "    \n",
    "with open('X_list.pkl', 'rb') as f:\n",
    "    X_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# PADDING WITH ZEROS ##########\n",
    "train_fin = train1\n",
    "noise_fin = noise1\n",
    "signal_fin = signal1\n",
    "X_fin = X_list\n",
    "\n",
    "# for i in range(len(train1)):\n",
    "#     z = np.zeros((178-len(train1[i]), 513))\n",
    "#     a  = np.concatenate((train1[i], z),axis = 0)\n",
    "#     train_fin.append(a)\n",
    "    \n",
    "#     z = np.zeros((178-len(signal1[i]), 513))\n",
    "#     a  = np.concatenate((signal1[i], z),axis = 0)\n",
    "#     signal_fin.append(a)\n",
    "    \n",
    "#     z = np.zeros((178-len(noise1[i]), 513))\n",
    "#     a  = np.concatenate((noise1[i], z),axis = 0)\n",
    "#     noise_fin.append(a)\n",
    "    \n",
    "    \n",
    "#     z = np.zeros((178-len(X_list[i]), 513),dtype = 'complex64')\n",
    "#     a  = np.concatenate((X_list[i], z),axis = 0)\n",
    "#     X_fin.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## IBM CREATION #######################\n",
    "ibm = []\n",
    "for i in range(len(signal_fin)):\n",
    "    ibm.append(np.greater(signal_fin[i],noise_fin[i])*1)\n",
    "    \n",
    "S_hat = []\n",
    "for i in range(1200):               \n",
    "    S_hat.append(np.multiply(ibm[i], X_fin[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ LOADING THE VALIDATION AUDIO CLIPS ###############\n",
    "\n",
    "train_val = []\n",
    "signal_val = []\n",
    "noise_val = []\n",
    "V_list = []\n",
    "s_clean = []\n",
    "\n",
    "import os, re\n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/v/vx*\")):\n",
    "        st, sr=librosa.load(f, sr=None)\n",
    "        V=librosa.stft(st, n_fft=1024, hop_length=512)\n",
    "        V_abs = np.abs(V)\n",
    "        train_val.append(np.transpose(V_abs))\n",
    "        V_list.append(np.transpose(V))\n",
    "        \n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/v/vn*\")):\n",
    "        sn, sr=librosa.load(f, sr=None)\n",
    "        V=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        V_abs = np.abs(V)\n",
    "        noise_val.append(np.transpose(V_abs))\n",
    "        \n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/v/vs*\")):\n",
    "        sc, sr=librosa.load(f, sr=None)\n",
    "        V=librosa.stft(sc, n_fft=1024, hop_length=512)\n",
    "        s_clean.append(V)\n",
    "        V_abs = np.abs(V)\n",
    "        signal_val.append(np.transpose(V_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SAVING THE FILES AS PICKLE #############\n",
    "\n",
    "# with open('train_val.pkl', 'wb') as f:\n",
    "#      pickle.dump(train_val, f)\n",
    "    \n",
    "# with open('noise_val.pkl', 'wb') as f:\n",
    "#      pickle.dump(noise_val, f)\n",
    "    \n",
    "# with open('signal_val.pkl', 'wb') as f:\n",
    "#      pickle.dump(signal_val, f)\n",
    "        \n",
    "# with open('V_list.pkl', 'wb') as f:\n",
    "#      pickle.dump(V_list, f) \n",
    "\n",
    "# with open('s_clean.pkl', 'wb') as f:\n",
    "#    pickle.dump(s_clean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### LOADING THE SAVED PICKLE FILES ###############\n",
    "\n",
    "with open('train_val.pkl', 'rb') as f:\n",
    "    train1_val = pickle.load(f)\n",
    "    \n",
    "with open('noise_val.pkl', 'rb') as f:\n",
    "    noise1_val = pickle.load(f)\n",
    "    \n",
    "with open('signal_val.pkl', 'rb') as f:\n",
    "    signal1_val = pickle.load(f)\n",
    "    \n",
    "with open('V_list.pkl', 'rb') as f:\n",
    "    V1_list = pickle.load(f)\n",
    "    \n",
    "with open('s_clean.pkl', 'rb') as f:\n",
    "    s1_clean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# IBM MATRIX FOR VALIDATION SET ##########\n",
    "\n",
    "ibm_val = []\n",
    "for i in range(len(signal_fin)):\n",
    "    ibm_val.append(np.greater(signal1_val[i],noise1_val[i])*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_arr = [len(x) for x in train1_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing  of Testing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ LOADING OF THE TEST DATA ##################\n",
    "test = []\n",
    "test_complex_list = []\n",
    "\n",
    "import os, re\n",
    "for f in sorted(glob.glob(\"/opt/e533/timit-homework/te/te*\")):\n",
    "        st, sr=librosa.load(f, sr=None)\n",
    "        X=librosa.stft(st, n_fft=1024, hop_length=512)\n",
    "        X_abs = np.abs(X)\n",
    "        test.append(np.transpose(X_abs))\n",
    "        test_complex_list.append(np.transpose(X))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### SAVING TEST DATA USING PICKLE ############\n",
    "# with open('test.pkl', 'wb') as f:\n",
    "#      pickle.dump(test, f)\n",
    "        \n",
    "# with open('test_complex.pkl', 'wb') as f:\n",
    "#      pickle.dump(test_complex_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ LOADING PICKLE FILES ##############\n",
    "\n",
    "with open('test_complex.pkl', 'rb') as f:\n",
    "    test_comp1 = pickle.load(f)\n",
    "    \n",
    "with open('test.pkl', 'rb') as f:\n",
    "    test1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = [len(x) for x in test1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Definition ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "dr = 0.9\n",
    "batch_size = 10\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 513 # MNIST data input (img shape: 28*28)\n",
    "num_layers = 2\n",
    "\n",
    "num_hidden = 513 \n",
    "num_classes = 513 \n",
    "\n",
    "# Input\n",
    "X = tf.placeholder(\"float\", [None,None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None,None, num_classes])\n",
    "dropout = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a 2-layered RNN is used for the purpose of denoising. Each cell is wrapped in a Dropout layer for the purpose of avoiding overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ RNN CELL STRUCTURE #################\n",
    "\n",
    "def RNN(X):\n",
    "\n",
    "    lstm_cells = []\n",
    "    \n",
    "    #Addition of Layers\n",
    "    for i in range(num_layers):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow and wrapping  it in Dropout \n",
    "        lstm_cell = rnn.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "        lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=dropout)\n",
    "        lstm_cells.append(lstm_cell)\n",
    "        \n",
    "    lstm_cell = rnn.MultiRNNCell(lstm_cells)\n",
    "    \n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 513)\n",
      "(?, 513)\n"
     ]
    }
   ],
   "source": [
    "################# LOSS FUNCTION AND OPTIMIZER DEFINITION ###################\n",
    "\n",
    "output, state = RNN(X)\n",
    "\n",
    "# Defining logits and output \n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(output, num_classes, activation_fn=None)\n",
    "prediction = logits\n",
    "\n",
    "flat_Y = tf.reshape(Y, [-1] + Y.shape.as_list()[2:])\n",
    "flat_logit = tf.reshape(logits, [-1] + logits.shape.as_list()[2:])\n",
    "\n",
    "\n",
    "\n",
    "# Defining loss and optimizer\n",
    "loss_op =tf.losses.mean_squared_error(predictions=flat_logit, labels=flat_Y)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "\n",
    "#Defining accuracy measure \n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "print(flat_Y.shape)\n",
    "print(flat_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 is:  27.7763442248106\n",
      "Loss at epoch 2 is:  21.63682208955288\n",
      "Loss at epoch 3 is:  20.0035180747509\n",
      "Loss at epoch 4 is:  19.092994891107082\n",
      "Loss at epoch 5 is:  18.468937255442142\n",
      "Loss at epoch 6 is:  17.980053022503853\n",
      "Loss at epoch 7 is:  17.5129998549819\n",
      "Loss at epoch 8 is:  17.203936092555523\n",
      "Loss at epoch 9 is:  16.8566512465477\n",
      "Loss at epoch 10 is:  16.567163921892643\n",
      "Loss at epoch 11 is:  16.312053076922894\n",
      "Loss at epoch 12 is:  16.100208692252636\n",
      "Loss at epoch 13 is:  15.836823277175426\n",
      "Loss at epoch 14 is:  15.584485962986946\n",
      "Loss at epoch 15 is:  15.501247107982635\n",
      "Loss at epoch 16 is:  15.33034285157919\n",
      "Loss at epoch 17 is:  15.058369942009449\n",
      "Loss at epoch 18 is:  14.870569966733456\n",
      "Loss at epoch 19 is:  14.705615170300007\n",
      "Loss at epoch 20 is:  14.557749480009079\n",
      "Loss at epoch 21 is:  14.551204204559326\n",
      "Loss at epoch 22 is:  14.472444474697113\n",
      "Loss at epoch 23 is:  14.240056686103344\n",
      "Loss at epoch 24 is:  14.086849249899387\n",
      "Loss at epoch 25 is:  13.94241814315319\n",
      "Loss at epoch 26 is:  13.757447488605976\n",
      "Loss at epoch 27 is:  13.6261912509799\n",
      "Loss at epoch 28 is:  13.535598412156105\n",
      "Loss at epoch 29 is:  13.411967664957047\n",
      "Loss at epoch 30 is:  13.294371277093887\n",
      "Loss at epoch 31 is:  13.192725382745266\n",
      "Optimization Finished!\n",
      "Starting Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/numpy/core/numeric.py:492: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed\n",
      "Testing Set Evaluation Started\n",
      "Testing Set Evaluation Completed!\n"
     ]
    }
   ],
   "source": [
    "############################# TRAINING OF THE NETWORK ##################################\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess: \n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #Training for 31 epochs \n",
    "    for epoch in range(31):\n",
    "        ep_loss = 0\n",
    "        for i in range(0, 1200,10):\n",
    "            batch_target = np.array(ibm[i:i+10])\n",
    "            batch_data = np.array(train_fin[i:i+10])\n",
    "            c,_ = sess.run([loss_op, optimizer], {X: batch_data, Y: batch_target, dropout: dr})\n",
    "            \n",
    "            ep_loss += c\n",
    "        print('Loss at epoch',epoch+1, 'is: ', ep_loss)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Using the model for Prediction of the Validation and Testing Sets\n",
    "    pred = []\n",
    "    pred_test = []\n",
    "    \n",
    "    \n",
    "    # Validation evaluation\n",
    "    print('Starting Evaluation')\n",
    "    for i in range(1200):\n",
    "        pred.append(sess.run(prediction, {X: np.reshape(V1_list[i],(-1,len_arr[i],513)), Y: np.reshape(ibm_val[i],(-1,len_arr[i],513)), dropout: dr} ))\n",
    "        \n",
    "    print('Evaluation completed')\n",
    "    \n",
    "    \n",
    "    # Testing of Data\n",
    "    print('Testing Set Evaluation Started')\n",
    "    for i in range(len(test1)):\n",
    "        pred_test.append(sess.run(prediction, {X: np.reshape(test_comp1[i],(-1,len_test[i],513)), dropout: dr} ))\n",
    "    \n",
    "    print('Testing Set Evaluation Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of the wave using the predicted Mask ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# CREATING S-HAT ##################\n",
    "M_hat = []\n",
    "M_list = []\n",
    "for i in range(1200):\n",
    "    M_hat.append(np.multiply(pred[i],V1_list[i]))\n",
    "    M_hat[i] = np.reshape(M_hat[i], (len_arr[i],513))\n",
    "    M_hat[i] = np.transpose(M_hat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################## WRITING THE NEW WAVE AS A .WAV FILE ########################\n",
    "s_hat = []\n",
    "\n",
    "for i in range(1200):\n",
    "    stest = librosa.istft(M_hat[i], hop_length = 512)\n",
    "    librosa.output.write_wav(('val_'+str(i)+'.wav'), stest,sr = 16000)\n",
    "    s_hat.append(stest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR Calculation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max SNR= 14.493186473846436\n",
      "Average SNR= 1.5729016558781344\n",
      "Min SNR= -11.681849956512451\n"
     ]
    }
   ],
   "source": [
    "snr = []\n",
    "\n",
    "for i in range(1200):\n",
    "    isclean =librosa.istft(s1_clean[i],hop_length = 512)\n",
    "\n",
    "    den = (isclean-s_hat[i])**2\n",
    "    num = (isclean)**2\n",
    "    \n",
    "    snr.append(10*(np.log10(np.sum(num)/np.sum(den))))\n",
    "\n",
    "print('Max SNR=', max(snr))\n",
    "print('Average SNR=', np.mean(snr))\n",
    "print('Min SNR=', min(snr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of Testing Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = []\n",
    "\n",
    "for i in range(400):\n",
    "    M_test.append(np.multiply(pred_test[i],test_comp1[i]))\n",
    "    M_test[i] = np.reshape(M_test[i], (len_test[i],513))\n",
    "    M_test[i] = np.transpose(M_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_hat = []\n",
    "\n",
    "for i in range(400):\n",
    "    stest = librosa.istft(M_test[i], hop_length = 512)\n",
    "    librosa.output.write_wav(('test_'+str(i)+'.wav'), stest,sr = 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "1. https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/\n",
    "\n",
    "2. https://github.com/aymericdamien/TensorFlow-Examples\n",
    "\n",
    "3. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
